{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.0.0\n",
      "sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.2\n",
      "numpy 1.18.5\n",
      "pandas 1.0.5\n",
      "sklearn 0.21.2\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "\n",
    "for module in mpl,np,pd,sklearn,tf,keras:\n",
    "    print(module.__name__,module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all,x_test,y_train_all,y_test = train_test_split(housing.data,housing.target,random_state=7)\n",
    "\n",
    "x_train,x_valid,y_train,y_vaild=train_test_split(x_train_all,y_train_all,random_state=11)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_valid.shape,y_vaild.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "saclar = StandardScaler()\n",
    "x_train_sacled = saclar.fit_transform(x_train)\n",
    "x_valid_sacled = saclar.transform(x_valid)\n",
    "x_test_sacled=saclar.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0\n",
      "train 1\n",
      "train 2\n",
      "train 3\n",
      "train 4\n",
      "train 5\n",
      "train 6\n",
      "train 7\n",
      "train 8\n",
      "train 9\n",
      "train 10\n",
      "train 11\n",
      "train 12\n",
      "train 13\n",
      "train 14\n",
      "train 15\n",
      "train 16\n",
      "train 17\n",
      "train 18\n",
      "train 19\n",
      "valid 0\n",
      "valid 1\n",
      "valid 2\n",
      "valid 3\n",
      "valid 4\n",
      "valid 5\n",
      "valid 6\n",
      "valid 7\n",
      "valid 8\n",
      "valid 9\n",
      "test 0\n",
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 4\n",
      "test 5\n",
      "test 6\n",
      "test 7\n",
      "test 8\n",
      "test 9\n"
     ]
    }
   ],
   "source": [
    "# 生成CSV文件\n",
    "output_dir = \"generate_csv\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "def save_to_csv(output_dir,data,name_prefix,header=None,n_parts=10):\n",
    "    path_format=os.path.join(output_dir,\"{}_{:02d}.csv\")\n",
    "    filenames = []\n",
    "    \n",
    "    for file_idx, row_indices in enumerate(\n",
    "                                np.array_split(np.arange(len(data)),n_parts)):\n",
    "        print(name_prefix,file_idx)\n",
    "        part_csv = path_format.format(name_prefix,file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        with open(part_csv,\"wt\",encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header+\"\\n\")\n",
    "            for row_index in row_indices:\n",
    "                f.write(\",\".join(\n",
    "                    [repr(col) for col in data[row_index]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filenames\n",
    "\n",
    "# merge合并数据\n",
    "train_data = np.c_[x_train_sacled,y_train]\n",
    "valid_data = np.c_[x_valid_sacled,y_vaild]\n",
    "test_data = np.c_[x_test_sacled,y_test]\n",
    "\n",
    "header_cols = housing.feature_names+[\"MidianHouseValue\"]\n",
    "header_str = \",\".join(header_cols)\n",
    "\n",
    "train_filenames = save_to_csv(output_dir,train_data,\"train\",header_str,n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir,valid_data,\"valid\",header_str,n_parts=10)\n",
    "test_filenames = save_to_csv(output_dir,test_data,\"test\",header_str,n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train filenames:\n",
      "['generate_csv\\\\train_00.csv',\n",
      " 'generate_csv\\\\train_01.csv',\n",
      " 'generate_csv\\\\train_02.csv',\n",
      " 'generate_csv\\\\train_03.csv',\n",
      " 'generate_csv\\\\train_04.csv',\n",
      " 'generate_csv\\\\train_05.csv',\n",
      " 'generate_csv\\\\train_06.csv',\n",
      " 'generate_csv\\\\train_07.csv',\n",
      " 'generate_csv\\\\train_08.csv',\n",
      " 'generate_csv\\\\train_09.csv',\n",
      " 'generate_csv\\\\train_10.csv',\n",
      " 'generate_csv\\\\train_11.csv',\n",
      " 'generate_csv\\\\train_12.csv',\n",
      " 'generate_csv\\\\train_13.csv',\n",
      " 'generate_csv\\\\train_14.csv',\n",
      " 'generate_csv\\\\train_15.csv',\n",
      " 'generate_csv\\\\train_16.csv',\n",
      " 'generate_csv\\\\train_17.csv',\n",
      " 'generate_csv\\\\train_18.csv',\n",
      " 'generate_csv\\\\train_19.csv']\n",
      "valid filenames:\n",
      "['generate_csv\\\\valid_00.csv',\n",
      " 'generate_csv\\\\valid_01.csv',\n",
      " 'generate_csv\\\\valid_02.csv',\n",
      " 'generate_csv\\\\valid_03.csv',\n",
      " 'generate_csv\\\\valid_04.csv',\n",
      " 'generate_csv\\\\valid_05.csv',\n",
      " 'generate_csv\\\\valid_06.csv',\n",
      " 'generate_csv\\\\valid_07.csv',\n",
      " 'generate_csv\\\\valid_08.csv',\n",
      " 'generate_csv\\\\valid_09.csv']\n",
      "test filenames:\n",
      "['generate_csv\\\\test_00.csv',\n",
      " 'generate_csv\\\\test_01.csv',\n",
      " 'generate_csv\\\\test_02.csv',\n",
      " 'generate_csv\\\\test_03.csv',\n",
      " 'generate_csv\\\\test_04.csv',\n",
      " 'generate_csv\\\\test_05.csv',\n",
      " 'generate_csv\\\\test_06.csv',\n",
      " 'generate_csv\\\\test_07.csv',\n",
      " 'generate_csv\\\\test_08.csv',\n",
      " 'generate_csv\\\\test_09.csv']\n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "\n",
    "import pprint\n",
    "print(\"train filenames:\")\n",
    "pprint.pprint(train_filenames)\n",
    "print(\"valid filenames:\")\n",
    "pprint.pprint(valid_filenames)\n",
    "print(\"test filenames:\")\n",
    "pprint.pprint(test_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'generate_csv\\\\train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'-0.8219588176953616,1.874166156711919,0.18212349433218608,-0.03170019246279883,-0.6011178900722581,-0.14337494105109344,1.0852205298015787,-0.8613994495208361,1.054', shape=(), dtype=string)\n",
      "tf.Tensor(b'-0.6672227549433569,-0.04823952235146133,0.34529405473316743,0.5382668657200925,1.8521839533415545,-0.0611253832474835,-0.8417093045554153,1.520484740533546,1.59', shape=(), dtype=string)\n",
      "tf.Tensor(b'0.04971034572063198,-0.8492418886278699,-0.06214699417830008,0.17878747064657746,-0.8025354230744277,0.0005066066922077538,0.6466457006743215,-1.1060793768010604,2.286', shape=(), dtype=string)\n",
      "tf.Tensor(b'2.51504373119231,1.0731637904355105,0.5574401201546321,-0.17273513019187772,-0.612912610473286,-0.01909156503651574,-0.5710993036045546,-0.027490309606616956,5.00001', shape=(), dtype=string)\n",
      "tf.Tensor(b'-1.0775077698160966,-0.44874070548966555,-0.5680568205591913,-0.14269262164909954,-0.09666677138213985,0.12326468238687088,-0.3144863716683942,-0.4818958888413162,0.978', shape=(), dtype=string)\n",
      "tf.Tensor(b'-0.46794146200516895,-0.9293421252555106,0.11909925912590703,-0.060470113038678074,0.30344643606811583,-0.021851890609536125,1.873722084296329,-1.0411642940532422,1.012', shape=(), dtype=string)\n",
      "tf.Tensor(b'1.5180511450515526,-0.5288409421173064,0.8102470510967439,-0.1921416982863481,0.44135393614167334,0.027335058055345158,-0.8183808561975836,0.8563535093443789,2.898', shape=(), dtype=string)\n",
      "tf.Tensor(b'-1.453851024367546,1.874166156711919,-1.1315714708271856,0.3611276016530489,-0.3978857847006997,-0.03273859332533962,-0.7390641317809511,0.646627857389904,1.875', shape=(), dtype=string)\n",
      "tf.Tensor(b'1.8444675088321243,0.5124621340420246,0.505783649224786,-0.20645711406004988,-0.021362018052499883,-0.05811312281214649,0.8332732875369839,-1.2658703497187516,4.513', shape=(), dtype=string)\n",
      "tf.Tensor(b'-1.2310715896684647,0.9129633171802288,-0.19194563416838628,0.1285146301786722,-0.18739538985158558,0.1460427975617358,-0.7857210284966175,0.656614793197258,0.953', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#  1. filename-> dataset\n",
    "#  2. read file ->dataset -> datasets -> merge\n",
    "#  3. parse csv\n",
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "for item in filename_dataset:\n",
    "    print(item)\n",
    "    \n",
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "        # header不要，跳过\n",
    "        lambda filename : tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers\n",
    ")\n",
    "for line in dataset.take(10):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=292, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=293, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=294, shape=(), dtype=int32, numpy=3>, <tf.Tensor: id=295, shape=(), dtype=int32, numpy=4>, <tf.Tensor: id=296, shape=(), dtype=int32, numpy=5>]\n"
     ]
    }
   ],
   "source": [
    "# tf.io.decode_scv(str, record_defaults)\n",
    "# 把一个字符串转换成Tensor列表\n",
    "sample_str =\"1,2,3,4,5\"\n",
    "record_defaults = [tf.constant(0, dtype=tf.int32)] * 5\n",
    "parsed_fields = tf.io.decode_csv(sample_str,record_defaults)\n",
    "print(parsed_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=303, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=304, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=305, shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: id=306, shape=(), dtype=string, numpy=b'4'>, <tf.Tensor: id=307, shape=(), dtype=float32, numpy=5.0>]\n"
     ]
    }
   ],
   "source": [
    "sample_str =\"1,2,3,4,5\"\n",
    "record_defaults = [\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    0,\n",
    "    np.nan,\n",
    "    \"hello\",\n",
    "    tf.constant([])\n",
    "] \n",
    "parsed_fields = tf.io.decode_csv(sample_str,record_defaults)\n",
    "print(parsed_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Field 4 is required but missing in record 0! [Op:DecodeCSV]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0303c1d8d506>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",,,,\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecord_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv_v2\u001b[1;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name)\u001b[0m\n\u001b[0;32m   2023\u001b[0m       \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2024\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2025\u001b[1;33m       \u001b[0mselect_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mselect_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2026\u001b[0m   )\n\u001b[0;32m   2027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv\u001b[1;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_delim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfield_delim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0muse_quote_delim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_quote_delim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             select_cols=select_cols, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m     75\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv_eager_fallback\u001b[1;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name, ctx)\u001b[0m\n\u001b[0;32m    149\u001b[0m   _result = _execute.execute(b\"DecodeCSV\", len(record_defaults),\n\u001b[0;32m    150\u001b[0m                              \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_inputs_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_attrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                              name=name)\n\u001b[0m\u001b[0;32m    152\u001b[0m   _execute.record_gradient(\n\u001b[0;32m    153\u001b[0m       \"DecodeCSV\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Field 4 is required but missing in record 0! [Op:DecodeCSV]"
     ]
    }
   ],
   "source": [
    "tf.io.decode_csv(\",,,,\",record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d4f753d49e2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1,2,3,4,5,6,7\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecord_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv_v2\u001b[1;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name)\u001b[0m\n\u001b[0;32m   2023\u001b[0m       \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2024\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2025\u001b[1;33m       \u001b[0mselect_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mselect_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2026\u001b[0m   )\n\u001b[0;32m   2027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv\u001b[1;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_delim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfield_delim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0muse_quote_delim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_quote_delim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             select_cols=select_cols, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m     75\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv_eager_fallback\u001b[1;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name, ctx)\u001b[0m\n\u001b[0;32m    149\u001b[0m   _result = _execute.execute(b\"DecodeCSV\", len(record_defaults),\n\u001b[0;32m    150\u001b[0m                              \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_inputs_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_attrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                              name=name)\n\u001b[0m\u001b[0;32m    152\u001b[0m   _execute.record_gradient(\n\u001b[0;32m    153\u001b[0m       \"DecodeCSV\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]"
     ]
    }
   ],
   "source": [
    "tf.io.decode_csv(\"1,2,3,4,5,6,7\",record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=340, shape=(8,), dtype=float32, numpy=\n",
       " array([-0.82195884,  1.8741661 ,  0.1821235 , -0.03170019, -0.6011179 ,\n",
       "        -0.14337493,  1.0852206 , -0.8613995 ], dtype=float32)>,\n",
       " <tf.Tensor: id=341, shape=(1,), dtype=float32, numpy=array([1.054], dtype=float32)>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_csv_line(line,n_fields):\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line,record_defaults=defs)\n",
    "    # 特征值和目标值转换成向量     \n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])\n",
    "    return x,y\n",
    "    \n",
    "parse_csv_line(b'-0.8219588176953616,1.874166156711919,0.18212349433218608,-0.03170019246279883,-0.6011178900722581,-0.14337494105109344,1.0852205298015787,-0.8613994495208361,1.054'\n",
    "              ,n_fields=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. filename-> dataset\n",
    "#  2. read file ->dataset -> datasets -> merge\n",
    "#  3. parse csv\n",
    "\n",
    "def csv_reader_dataset(filename,n_readers=5,batch_size=32,n_parse_theads=5,shuffle_buffer_size=10000):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
