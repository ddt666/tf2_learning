{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本生成案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.2\n",
      "numpy 1.18.5\n",
      "pandas 1.0.5\n",
      "sklearn 0.21.2\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115393\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "input_filepath = \"./data/shakespeare.txt\"\n",
    "text = open(input_filepath, 'r').read()\n",
    "\n",
    "print(len(text))\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# 1. generate vocab\n",
    "# 2. build mapping char->id\n",
    "# 3. data -> id_data\n",
    "# 4. abcd -> bcd<eos>\n",
    "\n",
    "# 生成词汇列表\n",
    "vocab = sorted(set(text))\n",
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "# 生成{单词:id}的字典\n",
    "char2idx = {char:idx for idx, char in enumerate(vocab)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx2char = np.array(vocab)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47]\n",
      "First Citi\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 生成莎士比亚文本中每个单词对应的id的ndarray\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text_as_int[0:10])\n",
    "print(text[0:10])\n",
    "\n",
    "# F对应id是18，i对应的id是47..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(id_text):\n",
    "    \"\"\"\n",
    "    把输入的文本拆分成特征值和目标值\n",
    "    abcde -> abcd, bcde\n",
    "    a的目标值是b\n",
    "    b的目标值是c\n",
    "    ...\n",
    "    \"\"\"\n",
    "    return id_text[0:-1], id_text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int32) F\n",
      "tf.Tensor(47, shape=(), dtype=int32) i\n"
     ]
    }
   ],
   "source": [
    "# 单词id的dataset\n",
    "char_dataset =tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "for ch_id in char_dataset.take(2):\n",
    "    print(ch_id, idx2char[ch_id.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'First '\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(\"\".join(idx2char[[18,47,56,57,58,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1], shape=(101,), dtype=int32)\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1]\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49], shape=(101,), dtype=int32)\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49]\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_length = 100\n",
    "\n",
    "# 转变成句子的dataset，长度+1：因为split_input_target函数输出的特征值和目标值会减1，所以+1\n",
    "# drop_remainder= True：最后一个batch不够长了就丢弃\n",
    "seq_dataset = char_dataset.batch(seq_length + 1,\n",
    "                                 drop_remainder = True)\n",
    "\n",
    "for seq_id in seq_dataset.take(2):\n",
    "    print(seq_id)\n",
    "    print(seq_id.numpy())\n",
    "    print(repr(''.join(idx2char[seq_id.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59  1]\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1]\n",
      "[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n",
      " 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n",
      " 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n",
      "  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n",
      " 53 59  1 49]\n"
     ]
    }
   ],
   "source": [
    "# 通过split_input_target函数生成包含特征值和目标值的dataset\n",
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "for item_input, item_output in seq_dataset.take(2):\n",
    "    print(item_input.numpy())\n",
    "    print(item_output.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "# 打乱数据集\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (64, None, 1024)          1311744   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 1,395,009\n",
      "Trainable params: 1,395,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                               batch_input_shape = [batch_size, None]),\n",
    "        keras.layers.SimpleRNN(units = rnn_units,\n",
    "                               # stateful = True,\n",
    "                               # recurrent_initializer = 'glorot_uniform',\n",
    "                               return_sequences = True),\n",
    "        keras.layers.Dense(vocab_size),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size = vocab_size,\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units,\n",
    "    batch_size = batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65)\n",
      "tf.Tensor(\n",
      "[[[ 0.00420645 -0.17076954  0.23231763 ...  0.27877596 -0.12193693\n",
      "    0.06219586]\n",
      "  [ 0.15314138 -0.1999325   0.18007226 ... -0.20383248 -0.17511083\n",
      "    0.08112726]\n",
      "  [-0.08200993 -0.11652202 -0.05141822 ...  0.08131687 -0.09853327\n",
      "    0.14707337]\n",
      "  ...\n",
      "  [-0.02426647  0.01919612  0.09885311 ...  0.12265646 -0.02642789\n",
      "    0.17074424]\n",
      "  [-0.12853238  0.0738643   0.16818419 ... -0.20715392  0.09658906\n",
      "   -0.13018838]\n",
      "  [ 0.21863244 -0.03189201  0.14359096 ...  0.20629099  0.02806747\n",
      "    0.08674693]]\n",
      "\n",
      " [[ 0.06538007  0.11084625 -0.22975498 ...  0.03824784  0.14862376\n",
      "   -0.02602544]\n",
      "  [-0.06358141 -0.01001276  0.10095014 ...  0.15676852 -0.2769231\n",
      "    0.07567746]\n",
      "  [ 0.09230173 -0.08547395  0.15885106 ...  0.26011205  0.01695213\n",
      "    0.21837787]\n",
      "  ...\n",
      "  [-0.27724606  0.21861666 -0.00730845 ... -0.09906368  0.16989529\n",
      "   -0.0506777 ]\n",
      "  [-0.10949579 -0.08770484  0.21917537 ...  0.21310665  0.1930785\n",
      "    0.04913554]\n",
      "  [ 0.18605033 -0.00887216  0.09398348 ...  0.0110951   0.30871227\n",
      "   -0.03104109]]\n",
      "\n",
      " [[ 0.04803687 -0.2552293   0.05521187 ...  0.1265826   0.2930414\n",
      "   -0.05313095]\n",
      "  [ 0.02749179 -0.08202448  0.2667629  ...  0.30281487 -0.17876355\n",
      "   -0.09544408]\n",
      "  [ 0.06961318 -0.10585697  0.20751357 ... -0.2086491   0.04844549\n",
      "   -0.2522808 ]\n",
      "  ...\n",
      "  [ 0.07991664  0.28596097 -0.09165797 ...  0.1078297  -0.13866708\n",
      "    0.00626023]\n",
      "  [-0.11433233  0.11177748  0.03566183 ... -0.12342555 -0.33863652\n",
      "    0.01236185]\n",
      "  [-0.08530968  0.04673089 -0.1121732  ...  0.18940088 -0.10730922\n",
      "    0.03677432]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.2809897  -0.24942417  0.21369037 ... -0.04002916  0.11849608\n",
      "   -0.16355148]\n",
      "  [-0.05421537  0.062648    0.1739337  ... -0.2069616   0.11046025\n",
      "   -0.22379848]\n",
      "  [-0.0409939   0.2268791   0.08748826 ...  0.2521884   0.06257188\n",
      "    0.0830131 ]\n",
      "  ...\n",
      "  [ 0.1650678   0.17864975  0.07925376 ...  0.1139328  -0.16417712\n",
      "   -0.15384778]\n",
      "  [-0.1491905   0.07167421 -0.17906067 ... -0.01775595  0.14327155\n",
      "   -0.21707633]\n",
      "  [ 0.36740324 -0.13088982  0.20811066 ...  0.0679044   0.18029188\n",
      "    0.01487304]]\n",
      "\n",
      " [[-0.01786724 -0.21177866  0.20061256 ...  0.05900447  0.2275557\n",
      "    0.07720904]\n",
      "  [ 0.04422679 -0.1645006   0.17551138 ...  0.15279451  0.10768013\n",
      "   -0.18004692]\n",
      "  [-0.23739317 -0.21535718  0.07780889 ... -0.05792658 -0.11370243\n",
      "   -0.17474982]\n",
      "  ...\n",
      "  [ 0.14805293 -0.04034699  0.16493656 ...  0.15145668  0.16321604\n",
      "   -0.08545084]\n",
      "  [-0.13889311 -0.05731726  0.05214751 ... -0.06449436  0.31960788\n",
      "   -0.16575593]\n",
      "  [ 0.26695144 -0.11209092 -0.14296345 ...  0.03633913 -0.02730892\n",
      "    0.02050236]]\n",
      "\n",
      " [[ 0.04957792 -0.09710706  0.0403219  ...  0.00089364 -0.25935352\n",
      "   -0.28634316]\n",
      "  [-0.13556752 -0.3354921   0.05262285 ...  0.16267624  0.03963155\n",
      "    0.17221218]\n",
      "  [-0.1212513   0.12491467 -0.03328368 ... -0.08245407 -0.11765754\n",
      "   -0.18245095]\n",
      "  ...\n",
      "  [ 0.1229641  -0.09370381  0.2536056  ... -0.03072144  0.30340868\n",
      "   -0.11817614]\n",
      "  [-0.00872299 -0.38746515  0.07557543 ... -0.14019029  0.18773726\n",
      "    0.11052805]\n",
      "  [ 0.01267007  0.03405804 -0.05102388 ...  0.26588523  0.00617055\n",
      "    0.0492442 ]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)\n",
    "    print(example_batch_predictions)\n",
    "\n",
    "# 生成概率分布，对应65个字符的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[19]\n",
      " [33]\n",
      " [58]\n",
      " [10]\n",
      " [45]\n",
      " [27]\n",
      " [18]\n",
      " [52]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [44]\n",
      " [54]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [47]\n",
      " [54]\n",
      " [15]\n",
      " [ 8]\n",
      " [43]\n",
      " [60]\n",
      " [22]\n",
      " [51]\n",
      " [34]\n",
      " [33]\n",
      " [47]\n",
      " [42]\n",
      " [13]\n",
      " [20]\n",
      " [36]\n",
      " [ 3]\n",
      " [62]\n",
      " [19]\n",
      " [48]\n",
      " [33]\n",
      " [56]\n",
      " [28]\n",
      " [ 1]\n",
      " [22]\n",
      " [50]\n",
      " [27]\n",
      " [54]\n",
      " [17]\n",
      " [ 8]\n",
      " [46]\n",
      " [20]\n",
      " [49]\n",
      " [ 7]\n",
      " [57]\n",
      " [28]\n",
      " [50]\n",
      " [19]\n",
      " [33]\n",
      " [62]\n",
      " [37]\n",
      " [23]\n",
      " [57]\n",
      " [53]\n",
      " [51]\n",
      " [60]\n",
      " [37]\n",
      " [15]\n",
      " [10]\n",
      " [11]\n",
      " [11]\n",
      " [28]\n",
      " [14]\n",
      " [27]\n",
      " [23]\n",
      " [49]\n",
      " [ 8]\n",
      " [54]\n",
      " [19]\n",
      " [25]\n",
      " [ 8]\n",
      " [58]\n",
      " [35]\n",
      " [47]\n",
      " [41]\n",
      " [11]\n",
      " [49]\n",
      " [61]\n",
      " [38]\n",
      " [53]\n",
      " [26]\n",
      " [39]\n",
      " [64]\n",
      " [62]\n",
      " [34]\n",
      " [19]\n",
      " [56]\n",
      " [14]\n",
      " [41]\n",
      " [25]\n",
      " [22]\n",
      " [47]\n",
      " [58]\n",
      " [54]\n",
      " [34]\n",
      " [35]\n",
      " [53]], shape=(100, 1), dtype=int64)\n",
      "tf.Tensor(\n",
      "[19 33 58 10 45 27 18 52  3  9 44 54  3  6 47 54 15  8 43 60 22 51 34 33\n",
      " 47 42 13 20 36  3 62 19 48 33 56 28  1 22 50 27 54 17  8 46 20 49  7 57\n",
      " 28 50 19 33 62 37 23 57 53 51 60 37 15 10 11 11 28 14 27 23 49  8 54 19\n",
      " 25  8 58 35 47 41 11 49 61 38 53 26 39 64 62 34 19 56 14 41 25 22 47 58\n",
      " 54 34 35 53], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 随机采样 random sampling.\n",
    "# greedy, random.\n",
    "sample_indices = tf.random.categorical(\n",
    "    logits = example_batch_predictions[0], num_samples = 1)\n",
    "print(sample_indices)\n",
    "# (100, 65) -> (100, 1)\n",
    "\n",
    "# 把1的维度消除\n",
    "sample_indices = tf.squeeze(sample_indices, axis = -1)\n",
    "print(sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  'the danger now,\\nFor suffering so the causes of our wreck.\\n\\nNORTHUMBERLAND:\\nNot so; even through the '\n",
      "\n",
      "Output:  'he danger now,\\nFor suffering so the causes of our wreck.\\n\\nNORTHUMBERLAND:\\nNot so; even through the h'\n",
      "\n",
      "Predictions:  'GUt:gOFn$3fp$,ipC.evJmVUidAHX$xGjUrP JlOpE.hHk-sPlGUxYKsomvYC:;;PBOKk.pGM.tWic;kwZoNazxVGrBcMJitpVWo'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Output: \", repr(\"\".join(idx2char[target_example_batch[0]])))\n",
    "print()\n",
    "print(\"Predictions: \", repr(\"\".join(idx2char[sample_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "4.191166\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    \"\"\"\n",
    "    自定义损失函数\n",
    "    \"\"\"\n",
    "    return keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = loss)\n",
    "example_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(example_loss.shape)\n",
    "print(example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "172/172 [==============================] - 23s 134ms/step - loss: 2.7345\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 2.0745\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.8422\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.6998\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.6076\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.5471\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.5023\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.4668\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.4386\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.4155\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.3967\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.3819\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.3649\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.3487\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.3372\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.3239\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.3127\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.3007\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.2901\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.2818\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.2702\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.2598\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.2526\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.2434\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.2329\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.2236\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.2152\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.2077\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.1983\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.1893\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.1828\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.1724\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.1658\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.1582\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - 21s 123ms/step - loss: 1.1504\n",
      "Epoch 36/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.1425\n",
      "Epoch 37/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.1369\n",
      "Epoch 38/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.1301\n",
      "Epoch 39/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.1220\n",
      "Epoch 40/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.1161\n",
      "Epoch 41/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.1113\n",
      "Epoch 42/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.1045\n",
      "Epoch 43/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0987\n",
      "Epoch 44/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0916\n",
      "Epoch 45/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0868\n",
      "Epoch 46/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0823\n",
      "Epoch 47/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0792\n",
      "Epoch 48/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0738\n",
      "Epoch 49/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0704\n",
      "Epoch 50/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0632\n",
      "Epoch 51/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0627\n",
      "Epoch 52/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0578\n",
      "Epoch 53/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0528\n",
      "Epoch 54/100\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.0514\n",
      "Epoch 55/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0474\n",
      "Epoch 56/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0457\n",
      "Epoch 57/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0430\n",
      "Epoch 58/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0398\n",
      "Epoch 59/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0383\n",
      "Epoch 60/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0350\n",
      "Epoch 61/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0340\n",
      "Epoch 62/100\n",
      "172/172 [==============================] - 21s 119ms/step - loss: 1.0310\n",
      "Epoch 63/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0345\n",
      "Epoch 64/100\n",
      "172/172 [==============================] - 21s 119ms/step - loss: 1.0274\n",
      "Epoch 65/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0271\n",
      "Epoch 66/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0267\n",
      "Epoch 67/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0258\n",
      "Epoch 68/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0263\n",
      "Epoch 69/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0273\n",
      "Epoch 70/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0237\n",
      "Epoch 71/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0247\n",
      "Epoch 72/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0267\n",
      "Epoch 73/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0243\n",
      "Epoch 74/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0258\n",
      "Epoch 75/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0233\n",
      "Epoch 76/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0238\n",
      "Epoch 77/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0288\n",
      "Epoch 78/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0288\n",
      "Epoch 79/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0281\n",
      "Epoch 80/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0272\n",
      "Epoch 81/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0296\n",
      "Epoch 82/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0298\n",
      "Epoch 83/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 1.0311\n",
      "Epoch 84/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0317\n",
      "Epoch 85/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0357\n",
      "Epoch 86/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0355\n",
      "Epoch 87/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0356\n",
      "Epoch 88/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0363\n",
      "Epoch 89/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0384\n",
      "Epoch 90/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0433\n",
      "Epoch 91/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0444\n",
      "Epoch 92/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0451\n",
      "Epoch 93/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0446\n",
      "Epoch 94/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0476\n",
      "Epoch 95/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0505\n",
      "Epoch 96/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0519\n",
      "Epoch 97/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0553\n",
      "Epoch 98/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0585\n",
      "Epoch 99/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 1.0587\n",
      "Epoch 100/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.0631\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./text_generation_checkpoints\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "checkpoint_prefix = os.path.join(output_dir, 'ckpt_{epoch}')\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True)\n",
    "\n",
    "epochs = 100\n",
    "history = model.fit(seq_dataset, epochs = epochs,\n",
    "                    callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text_generation_checkpoints\\\\ckpt_100'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看最新保存的模型\n",
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (1, None, 1024)           1311744   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 1,395,009\n",
      "Trainable params: 1,395,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 载入模型\n",
    "model2 = build_model(vocab_size,\n",
    "                     embedding_dim,\n",
    "                     rnn_units,\n",
    "                     batch_size = 1)\n",
    "\n",
    "# 从模型中载入权重参数\n",
    "model2.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model2.build(tf.TensorShape([1, None]))\n",
    "# start ch sequence A,\n",
    "# A -> model -> b\n",
    "# A.append(b) -> B\n",
    "# B(Ab) -> model -> c\n",
    "# B.append(c) -> C\n",
    "# C(Abc) -> model -> ...\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: I neg his accusation\n",
      "At all this?\n",
      "\n",
      "BENVOLIO:\n",
      "By good my liege!\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Farewell:\n",
      "And I fear Tauntle.\n",
      "He good my part,\n",
      "And in his ancient cheeks,\n",
      "Since beseech you, north,\n",
      "Where on thy that is my name, and still hid to our hearts\n",
      "silence is\n",
      "Anot.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Ghoot.' Your cousin! were he will catches to give or nurse, the happy its:\n",
      "I will; and, as I give you.\n",
      "\n",
      "ANTH:\n",
      "Then abstinence!\n",
      "Thy wife and instantless heaven of Polide and deep has the servants?\n",
      "\n",
      "POLIXENES:\n",
      "What blessed men can lost where they can then be nothing!\n",
      "Have chopping ugams,\n",
      "Follows to the branches behalf:\n",
      "My master Katharina, and ten times with an envy more power.\n",
      "\n",
      "MERCUTIO:\n",
      "Come, blard;\n",
      "But I, this ill that you read true fair!\n",
      "Why should I no more and honour well\n",
      "When will I determy daughter, and I will not, nor an 'tis rubenish departing in my heart but fool and dispatched cry?\n",
      "\n",
      "KING RICHARD II:\n",
      "To tell thee, ne'er hides our course here,\n",
      "It is the issue of but and old good counsel, will you chose this land\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, num_generate = 1000):\n",
    "    input_eval = [char2idx[ch] for ch in start_string]\n",
    "    # 维度扩展\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        # 1. model inference -> predictions\n",
    "        # 2. sample -> ch -> text_generated.\n",
    "        # 3. update input_eval\n",
    "        \n",
    "        # predictions : [batch_size, input_eval_len, vocab_size]\n",
    "        predictions = model(input_eval)\n",
    "        # predictions : [input_eval_len, vocab_size]\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # predicted_ids: [input_eval_len, 1]\n",
    "        # a b c -> b c d\n",
    "        predicted_id = tf.random.categorical(\n",
    "            predictions, num_samples = 1)[-1, 0].numpy()\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        # s, x -> rnn -> s', y\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "new_text = generate_text(model2, \"All: \")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
